@string{jgr = "References"}
@book{johnson_1997, place={Cambridge}, edition={2}, series={London Mathematical Society Student Texts}, title={Presentations of Groups}, DOI={10.1017/CBO9781139168410}, publisher={Cambridge University Press}, author={Johnson, D. L.}, year={1997}, collection={London Mathematical Society Student Texts}}

@article{BOURNEZ2007317,
	title = {Polynomial differential equations compute all real computable functions on computable compact intervals},
	journal = {Journal of Complexity},
	volume = {23},
	number = {3},
	pages = {317-335},
	year = {2007},
	issn = {0885-064X},
	doi = {https://doi.org/10.1016/j.jco.2006.12.005},
	url = {https://www.sciencedirect.com/science/article/pii/S0885064X07000246},
	author = {Olivier Bournez and Manuel L. Campagnolo and Daniel S. Graça and Emmanuel Hainry},
	keywords = {Analog computation, Computable analysis, General Purpose Analog Computer, Church–Turing thesis, Differential equations}
}

@Inbook{Bournez2008,
	author="Bournez, Olivier
	and Campagnolo, Manuel L.",
	editor="Cooper, S. Barry
	and L{\"o}we, Benedikt
	and Sorbi, Andrea",
	title="A Survey on Continuous Time Computations",
	bookTitle="New Computational Paradigms: Changing Conceptions of What is Computable",
	year="2008",
	publisher="Springer New York",
	address="New York, NY",
	pages="383--423",
	abstract="We provide an overview of theories of continuous time computation. These theories allow us to understand both the hardness of questions related to continuous time dynamical systems and the computational power of continuous time analog models. We survey the existing models, summarizing results, and point to relevant references in the literature.",
	isbn="978-0-387-68546-5",
	doi="10.1007/978-0-387-68546-5_17",
	url="https://doi.org/10.1007/978-0-387-68546-5_17"
}



@article{Swadlow1994,
	author = {Swadlow, H. A.},
	title = {Efferent neurons and suspected interneurons in motor cortex of the awake rabbit: Axonal properties, sensory receptive fields, and subthreshold synaptic inputs},
	journal = {Journal of Neurophysiology},
	volume = {71},
	number = {2},
	pages = {437–453},
	year = {1994}
}

@article{Swadlow1992,
	author = {Swadlow, H. A.},
	title = {Monitoring the excitability of neocortical efferent neurons to direct activation by extracellular current pulses},
	journal = {Journal of Neurophysiology},
	volume = {68},
	number = {2},
	pages = {605-619},
	year = {1992},
	doi = {10.1152/jn.1992.68.2.605},
	note ={PMID: 1527578},
	
	URL = { 
	https://doi.org/10.1152/jn.1992.68.2.605
	
	},
	eprint = { 
	https://doi.org/10.1152/jn.1992.68.2.605
	
	}
}


@article{Swadlow1988,
	author = {Swadlow, H. A.},
	title = {Efferent neurons and suspected interneurons in binocular visual cortex of the awake rabbit: receptive fields and binocular properties},
	journal = {Journal of Neurophysiology},
	volume = {59},
	number = {4},
	pages = {1162-1187},
	year = {1988},
	doi = {10.1152/jn.1988.59.4.1162},
	note ={PMID: 3373273},
	
	URL = { 
	https://doi.org/10.1152/jn.1988.59.4.1162
	
	},
	eprint = { 
	https://doi.org/10.1152/jn.1988.59.4.1162
	
	}
}

@article{Swadlow1985,
	author = {Swadlow, H. A. and Weyand, T. G.},
	title = {Receptive-field and axonal properties of neurons in the dorsal lateral geniculate nucleus of awake unparalyzed rabbits},
	journal = {Journal of Neurophysiology},
	volume = {54},
	number = {1},
	pages = {168-183},
	year = {1985},
	doi = {10.1152/jn.1985.54.1.168},
	note ={PMID: 2993538},
	
	URL = { 
	https://doi.org/10.1152/jn.1985.54.1.168
	
	},
	eprint = { 
	https://doi.org/10.1152/jn.1985.54.1.168
	
	}
}

@article{villa,
	author = {Alessandro E. P. Villa  and Igor V. Tetko  and Brian Hyland  and Abdellatif Najem },
	title = {Spatiotemporal activity patterns of rat cortical neurons predict responses in a conditioned task},
	journal = {Proceedings of the National Academy of Sciences},
	volume = {96},
	number = {3},
	pages = {1106-1111},
	year = {1999},
	doi = {10.1073/pnas.96.3.1106},
	URL = {https://www.pnas.org/doi/abs/10.1073/pnas.96.3.1106},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.96.3.1106}
}
@article{tetko,
	author = {Tetko, Igor and Villa, Alessandro},
	year = {2001},
	month = {02},
	pages = {1-14},
	title = {A pattern grouping algorithm for analysis of spatiotemporal patterns in neuronal spike trains. 1. Detection of repeated patterns},
	volume = {105},
	journal = {Journal of neuroscience methods},
	doi = {10.1016/s0165-0270(00)00336-8}
}

@article{prut,
	author = {Prut, Yifat and Vaadia, Eilon and Bergman, Hagai and Haalman, Iris and Slovin, Hamutal and Abeles, Moshe},
	title = {Spatiotemporal Structure of Cortical Activity: Properties and Behavioral Relevance},
	journal = {Journal of Neurophysiology},
	volume = {79},
	number = {6},
	pages = {2857-2874},
	year = {1998},
	doi = {10.1152/jn.1998.79.6.2857},
	note ={PMID: 9636092},
	
	URL = { 
	https://doi.org/10.1152/jn.1998.79.6.2857
	
	},
	eprint = { 
	https://doi.org/10.1152/jn.1998.79.6.2857
	
	}
}

@article{lindsey,
	author = {Lindsey, B. G. and Morris, K. F. and Shannon, R. and Gerstein, G. L.},
	title = {Repeated Patterns of Distributed Synchrony in Neuronal Assemblies},
	journal = {Journal of Neurophysiology},
	volume = {78},
	number = {3},
	pages = {1714-1719},
	year = {1997},
	doi = {10.1152/jn.1997.78.3.1714},
	note ={PMID: 9310455},
    URL = {https://doi.org/10.1152/jn.1997.78.3.1714},
	eprint = { https://doi.org/10.1152/jn.1997.78.3.1714}
}


@book{deLaHiguera,
	author = {de la Higuera, Colin},
	title = {Grammatical Inference: Learning Automata and Grammars},
	year = {2010},
	isbn = {0521763169},
	publisher = {Cambridge University Press},
	address = {USA}
}
@inproceedings{Shen2017KolmogorovCA,
	title={Kolmogorov Complexity and Algorithmic Randomness},
	author={Alexander Shen and Vladimir A. Uspensky and Nikolai K. Vereshchagin},
	year={2017}
}


@article{SOLOMONOFF19641,
	title = {A formal theory of inductive inference. Part I},
	journal = {Information and Control},
	volume = {7},
	number = {1},
	pages = {1-22},
	year = {1964},
	issn = {0019-9958},
	doi = {https://doi.org/10.1016/S0019-9958(64)90223-2},
	url = {https://www.sciencedirect.com/science/article/pii/S0019995864902232},
	author = {R.J. Solomonoff}
}

@article{inhibitory_plasticity,
	author = {Vogels TP and Froemke RC and Doyon N },
	journal = {Front Neural Circuits},
	publisher = {Trends in cognitive sciences},
	title = {Inhibitory synaptic plasticity: spike timing-dependence and putative network function},
	year = {2013},
	doi = {10.3389/fncir.2013.00119}
}

@article{free_energy_principle_and_brain,
	author = {Friston, Karl},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {7},
	pages = {293-301},
	publisher = {Trends in cognitive sciences},
	title = {The free-energy principle: a rough guide to the brain?},
	volume = {13},
	year = {2009},
	doi = {10.1016/j.tics.2009.04.005}
}

@article{em_alg,
	ISSN = {00359246},
	URL = {http://www.jstor.org/stable/2984875},
	abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
	author = {A. P. Dempster and N. M. Laird and D. B. Rubin},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {1},
	pages = {1--38},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {Maximum Likelihood from Incomplete Data via the EM Algorithm},
	urldate = {2022-06-25},
	volume = {39},
	year = {1977}
}


@inproceedings{TEM_as_teransformer,
	title={Relating transformers to models and neural representations of the hippocampal formation},
	author={James C. R. Whittington and Joseph Warren and Tim E.J. Behrens},
	booktitle={International Conference on Learning Representations},
	year={2022},
	url={https://openreview.net/forum?id=B8DVo9B1YE0}
}
@article{TEM,
	title = {The Tolman-Eichenbaum Machine: Unifying Space and Relational Memory through Generalization in the Hippocampal Formation},
	journal = {Cell},
	volume = {183},
	number = {5},
	pages = {1249-1263.e23},
	year = {2020},
	issn = {0092-8674},
	doi = {https://doi.org/10.1016/j.cell.2020.10.024},
	url = {https://www.sciencedirect.com/science/article/pii/S009286742031388X},
	author = {James C.R. Whittington and Timothy H. Muller and Shirley Mark and Guifen Chen and Caswell Barry and Neil Burgess and Timothy E.J. Behrens},
	keywords = {hippocampus, entorhinal cortex, generalization, grid cells, place cells, neural networks, non-spatial reasoning, representation learning},
	abstract = {Summary
	The hippocampal-entorhinal system is important for spatial and relational memory tasks. We formally link these domains, provide a mechanistic understanding of the hippocampal role in generalization, and offer unifying principles underlying many entorhinal and hippocampal cell types. We propose medial entorhinal cells form a basis describing structural knowledge, and hippocampal cells link this basis with sensory representations. Adopting these principles, we introduce the Tolman-Eichenbaum machine (TEM). After learning, TEM entorhinal cells display diverse properties resembling apparently bespoke spatial responses, such as grid, band, border, and object-vector cells. TEM hippocampal cells include place and landmark cells that remap between environments. Crucially, TEM also aligns with empirically recorded representations in complex non-spatial tasks. TEM also generates predictions that hippocampal remapping is not random as previously believed; rather, structural knowledge is preserved across environments. We confirm this structural transfer over remapping in simultaneously recorded place and grid cells.}
}
@article{sfa_complex_cells,
	author = {Berkes, Pietro and Wiskott, Laurenz},
	year = {2005},
	month = {02},
	pages = {579-602},
	title = {Slow feature analysis yields a rich repertoire of complex cell properties},
	volume = {5},
	journal = {Journal of vision},
	doi = {10.1167/5.6.9}
}
@article{slow_feature_analysis,
	title={Slow Feature Analysis: Unsupervised Learning of Invariances},
	author={Laurenz Wiskott and Terrence J. Sejnowski},
	journal={Neural Computation},
	year={2002},
	volume={14},
	pages={715-770}
}
@article{hipp_as_cog_map,
	author = {Stachenfeld, K. AND Botvinick, M. AND Gershman, S},
	title = {The hippocampus as a predictive map},
	journal = {Nature Neuroscience},
	volume = {20},
	pages = {1643–1653},
	year = {2017},
	doi = {10.1038/nn.4650}
}
@article{Kurpic,
	author = {Krupic, Julija et al.},
	title = {Grid cell symmetry is shaped by environmental geometry},
	journal = {Nature},
	volume = {7538},
	number = {518},
	pages = {232-235},
	year = {2015},
	doi = {10.1038/nature14153}
}
@article{BVC,
	author = {O'Keefe, J. AND Burgess N.},
	title = {Geometric determinants of the place fields of hippocampal neurons},
	journal = {Nature},
	volume = {381},
	number = {2},
	pages = {425–428},
	year = {1996},
	doi = {10.1038/381425a0}
}
@article{slowness_sparseness,
	doi = {10.1371/journal.pcbi.0030166},
	author = {Franzius, Mathias AND Sprekeler, Henning AND Wiskott, Laurenz},
	journal = {PLOS Computational Biology},
	publisher = {Public Library of Science},
	title = {Slowness and Sparseness Lead to Place, Head-Direction, and Spatial-View Cells},
	year = {2007},
	month = {08},
	volume = {3},
	url = {https://doi.org/10.1371/journal.pcbi.0030166},
	pages = {1-18},
	abstract = {We present a model for the self-organized formation of place cells, head-direction cells, and spatial-view cells in the hippocampal formation based on unsupervised learning on quasi-natural visual stimuli. The model comprises a hierarchy of Slow Feature Analysis (SFA) nodes, which were recently shown to reproduce many properties of complex cells in the early visual system [1]. The system extracts a distributed grid-like representation of position and orientation, which is transcoded into a localized place-field, head-direction, or view representation, by sparse coding. The type of cells that develops depends solely on the relevant input statistics, i.e., the movement pattern of the simulated animal. The numerical simulations are complemented by a mathematical analysis that allows us to accurately predict the output of the top SFA layer.},
	number = {8},
	
}
@article{HD_cells,
	author = {Taube, J S et al},
	title = {Head-direction cells recorded from the postsubiculum in freely moving rats. I. Description and quantitative analysis},
	journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
	volume = {10},
	number = {2},
	pages = {420-35},
	year = {1990},
	doi = {10.1523/JNEUROSCI.10-02-00420.1990}
}
@article{Hafting2005,
	author = {Hafting, Torkel et al},
	title = {Microstructure of a spatial map in the entorhinal cortex},
	journal = {Nature},
	volume = {436},
	number = {7052},
	pages = {801-6},
	year = {2005},
	issn = {0899-7667},
	doi = {10.1038/nature03721}
}
@article{OKEEFE1971171,
	title = {The hippocampus as a spatial map. Preliminary evidence from unit activity in the freely-moving rat},
	journal = {Brain Research},
	volume = {34},
	number = {1},
	pages = {171-175},
	year = {1971},
	issn = {0006-8993},
	doi = {https://doi.org/10.1016/0006-8993(71)90358-1},
	url = {https://www.sciencedirect.com/science/article/pii/0006899371903581},
	author = {J. O'Keefe and J. Dostrovsky}
}
@article{cog_maps_tolman,
	author = {Tolman, E. C.},
	title = {Cognitive maps in rats and men},
	journal = {Psychological Review},
	volume = {55},
	number = {4},
	pages = {189–208},
	year = {1948},
	issn = {0899-7667},
	doi = {10.1037/h0061626}
}
@article{pred_coding_comp_graph,
	author = {Millidge, Beren and Tschantz, Alexander and Buckley, Christopher L.},
	title = "{Predictive Coding Approximates Backprop Along Arbitrary Computation Graphs}",
	journal = {Neural Computation},
	volume = {34},
	number = {6},
	pages = {1329-1368},
	year = {2022},
	month = {05},
	issn = {0899-7667},
	doi = {10.1162/neco_a_01497},
	url = {https://doi.org/10.1162/neco\_a\_01497},
	eprint = {https://direct.mit.edu/neco/article-pdf/34/6/1329/2023477/neco\_a\_01497.pdf},
}
@book{rl_sutton,
	author = {Sutton, Richard S. and Barto, Andrew G.},
	title = {Reinforcement Learning: An Introduction},
	year = {2018},
	isbn = {0262039249},
	publisher = {A Bradford Book},
	address = {Cambridge, MA, USA},
	abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.}
}
@article{lipschitz,
	author = {Luxburg, Ulrike von and Bousquet, Olivier},
	title = {Distance--Based Classification with Lipschitz Functions},
	year = {2004},
	issue_date = {12/1/2004},
	publisher = {JMLR.org},
	volume = {5},
	issn = {1532-4435},
	abstract = {The goal of this article is to develop a framework for large margin classification in metric spaces. We want to find a generalization of linear decision functions for metric spaces and define a corresponding notion of margin such that the decision function separates the training points with a large margin. It will turn out that using Lipschitz functions as decision functions, the inverse of the Lipschitz constant can be interpreted as the size of a margin. In order to construct a clean mathematical setup we isometrically embed the given metric space into a Banach space and the space of Lipschitz functions into its dual space. To analyze the resulting algorithm, we prove several representer theorems. They state that there always exist solutions of the Lipschitz classifier which can be expressed in terms of distance functions to training points. We provide generalization bounds for Lipschitz classifiers in terms of the Rademacher complexities of some Lipschitz function classes. The generality of our approach can be seen from the fact that several well-known algorithms are special cases of the Lipschitz classifier, among them the support vector machine, the linear programming machine, and the 1-nearest neighbor classifier.},
	journal = {J. Mach. Learn. Res.},
	month = {dec},
	pages = {669–695},
	numpages = {27}
}
@misc{geom_deep_learn,
	doi = {10.48550/ARXIV.2104.13478},
	
	url = {https://arxiv.org/abs/2104.13478},
	
	author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
	
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computational Geometry (cs.CG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges},
	
	publisher = {arXiv},
	
	year = {2021},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}


@book{neuronal_dynamics,
	author = {Gerstner, Wulfram and Kistler, Werner M. and Naud, Richard and Paninski, Liam},
	title = {Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition},
	year = {2014},
	isbn = {1107635195},
	publisher = {Cambridge University Press},
	address = {USA},
	abstract = {What happens in our brain when we make a decision? What triggers a neuron to send out a signal? What is the neural code? This textbook for advanced undergraduate and beginning graduate students provides a thorough and up-to-date introduction to the fields of computational and theoretical neuroscience. It covers classical topics, including the Hodgkin-Huxley equations and Hopfield model, as well as modern developments in the field such as Generalized Linear Models and decision theory. Concepts are introduced using clear step-by-step explanations suitable for readers with only a basic knowledge of differential equations and probabilities, and are richly illustrated by figures and worked-out examples. End-of-chapter summaries and classroom-tested exercises make the book ideal for courses or for self-study. The authors also give pointers to the literature and an extensive bibliography, which will prove invaluable to readers interested in further study.}
}

@article{pred_coding,
	title = {A tutorial on the free-energy framework for modelling perception and learning},
	journal = {Journal of Mathematical Psychology},
	volume = {76},
	pages = {198-211},
	year = {2017},
	note = {Model-based Cognitive Neuroscience},
	issn = {0022-2496},
	doi = {https://doi.org/10.1016/j.jmp.2015.11.003},
	url = {https://www.sciencedirect.com/science/article/pii/S0022249615000759},
	author = {Rafal Bogacz}
}
@ARTICLE{var_inf,
	
	author={Tzikas, Dimitris G. and Likas, Aristidis C. and Galatsanos, Nikolaos P.},
	
	journal={IEEE Signal Processing Magazine}, 
	
	title={The variational approximation for Bayesian inference}, 
	
	year={2008},
	
	volume={25},
	
	number={6},
	
	pages={131-146},
	
	doi={10.1109/MSP.2008.929620}}


@article{predictive_map,
	author = {Stachenfeld Kimberly L AND Botvinick Matthew M},
	title = {The hippocampus as a predictive map},
	journal = {Nature Neuroscience},
	doi = {10.1038/nn.4650},
	year = {2017}
}

@article{past_and_future,
	author = {Schacter, Daniel L. and Donna Rose Addis},
	title = {The cognitive neuroscience of constructive memory: remembering the past and imagining the future.},
	journal = {Philosophical transactions of the Royal Society of London},
	doi = {10.1098/rstb.2007.2087},
	year = {2007}
}

@article{
	alternating_sequences,
	author = {Mengni Wang  and David J. Foster  and Brad E. Pfeiffer },
	title = {Alternating sequences of future and past behavior encoded within hippocampal theta oscillations},
	journal = {Science},
	volume = {370},
	number = {6513},
	pages = {247-250},
	year = {2020},
	doi = {10.1126/science.abb4151},
	URL = {https://www.science.org/doi/abs/10.1126/science.abb4151},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.abb4151}
}

@article{shard_wave_ripple,
	author = {Buzsáki G.},
	title = {Hippocampal sharp wave-ripple: A cognitive biomarker for episodic memory and planning},
	journal = {Hippocampus},
	doi = {10.1002/hipo.22488},
	year = {2015}
}

@article{place_cell_sequences,
	author = {Pfeiffer Brad E. AND Foster David J.},
	title = {Hippocampal place-cell sequences depict future paths to remembered goals},
	journal = {Nature},
	volume = {497},
	doi = {10.1038/nature12112},
	year = {2013}
}

@article{hippocampus_abstract_geometry,
	author = {Nieh Edward H AND Schottdorf Manuel},
	title = {Geometry of abstract learned knowledge in the hippocampus},
	journal = {Nature},
	volume = {595},
	doi = {10.1038/s41586-021-03652-7},
	year = {2021}
}

@article{grid_cells_3d,
	author = {Grieves, Roddy M.},
	title = {The place-cell representation of volumetric space in rats},
	journal = {Nature Communications},
	volume = {11},
	doi = {10.1038/s41467-020-14611-7},
	year = {2020}
}


@article{vector_trace_cells,
	author = {Poulter, Steven et al},
	title = {Vector trace cells in the subiculum of the hippocampal formation.},
	journal = {Nature neuroscience},
	volume = {24,2},
	number = {4},
	pages = {266-275},
	doi = {10.1038/s41593-020-00761-w},
	year = {2021}
}

@misc{rlblogpost,
	title={Deep Reinforcement Learning Doesn't Work Yet},
	author={Irpan, Alex},
	howpublished={\url{https://www.alexirpan.com/2018/02/14/rl-hard.html}},
	year={2018}
}
@misc{vime,
	doi = {10.48550/ARXIV.1605.09674},
	
	url = {https://arxiv.org/abs/1605.09674},
	
	author = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
	
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Robotics (cs.RO), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {VIME: Variational Information Maximizing Exploration},
	
	publisher = {arXiv},
	
	year = {2016},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{generalisation_in_rl,
	doi = {10.48550/ARXIV.1711.00832},
	
	url = {https://arxiv.org/abs/1711.00832},
	
	author = {Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and Perolat, Julien and Silver, David and Graepel, Thore},
	
	keywords = {Artificial Intelligence (cs.AI), Computer Science and Game Theory (cs.GT), Machine Learning (cs.LG), Multiagent Systems (cs.MA), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning},
	
	publisher = {arXiv},
	
	year = {2017},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{rainbow_dqn,
	doi = {10.48550/ARXIV.1710.02298},
	
	url = {https://arxiv.org/abs/1710.02298},
	
	author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
	
	keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
	
	publisher = {arXiv},
	
	year = {2017},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{hodgkin,
	author = {Hodgkin, A. L. and Huxley, A. F.},
	title = {A quantitative description of membrane current and its application to conduction and excitation in nerve},
	journal = {The Journal of Physiology},
	volume = {117},
	number = {4},
	pages = {500-544},
	doi = {https://doi.org/10.1113/jphysiol.1952.sp004764},
	url = {https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1952.sp004764},
	eprint = {https://physoc.onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.1952.sp004764},
	year = {1952}
}

@inproceedings{maxq,
	author = {Dietterich, Thomas G.},
	title = {The MAXQ Method for Hierarchical Reinforcement Learning},
	year = {1998},
	isbn = {1558605568},
	publisher = {Morgan Kaufmann Publishers Inc.},
	address = {San Francisco, CA, USA},
	booktitle = {Proceedings of the Fifteenth International Conference on Machine Learning},
	pages = {118–126},
	numpages = {9},
	series = {ICML '98}
}
@inproceedings{parti_game,
	author = {Moore, A.W., Atkeson, C.G. },
	title = {The parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces. },
	year = {1995},
	publisher = {Springer},
	pages= {199–233},
	series = {Mach Learn 21}
}
@inbook{dyna,
	author = {Sutton, Richard S.},
	title = {First Results with Dyna, an Integrated Architecture for Learning, Planning and Reacting},
	year = {1990},
	isbn = {0262132613},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
	booktitle = {Neural Networks for Control},
	pages = {179–189},
	numpages = {11}
}
@online{pattern_sep,
	title = {Aging and the role of human hippocamal subfields in pattern separation},
	date = {2019},
	organization = {Center for the neurobiology of learning and memory},
	author = {Craig Stark}
}
@inproceedings{dg_pattern_sep,
	author = {McClelland JL },
	title = {Hippocampal conjunctive encoding, storage, and recall: avoiding a trade-off.},
	year = {1994},
	publisher = {O'Reilly RC},
	pages= {661-682},
	series = {Hippocampus}
}

@article{orientation_sensitive_cells,
	abstract = {A nerve net model for the visual cortex of higher vertebrates is presented. A simple learning procedure is shown to be sufficient for the organization of some essential functional properties of single units. The rather special assumptions usually made in the literature regarding preorganization of the visual cortex are thereby avoided. The model consists of 338 neurones forming a sheet analogous to the cortex. The neurones are connected randomly to a ``retina''of 19 cells. Nine different stimuli in the form of light bars were applied. The afferent connections were modified according to a mechanism of synaptic training. After twenty presentations of all the stimuli individual cortical neurones became sensitive to only one orientation. Neurones with the same or similar orientation sensitivity tended to appear in clusters, which are analogous to cortical columns. The system was shown to be insensitive to a background of disturbing input excitations during learning. After learning it was able to repair small defects introduced into the wiring and was relatively insensitive to stimuli not used during training.},
	author = {von der Malsburg, Chr. },
	da = {1973/06/01},
	date-added = {2021-11-18 13:47:03 +0100},
	date-modified = {2021-11-18 13:47:03 +0100},
	doi = {10.1007/BF00288907},
	id = {von der Malsburg1973},
	isbn = {1432-0770},
	journal = {Kybernetik},
	number = {2},
	pages = {85--100},
	title = {Self-organization of orientation sensitive cells in the striate cortex},
	ty = {JOUR},
	url = {https://doi.org/10.1007/BF00288907},
	volume = {14},
	year = {1973},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF00288907}}

@article{Recurrent_grid,
	abstract = {Using paired recordings from rat entorhinal stellate cells and computational modeling, this study shows that stellate cells in the medial entorhinal cortex (MEC) are almost exclusively connected to each other via inhibitory interneurons in an all-or-none style and that stable grid firing can arise from this recurrent inhibitory circuitry within the MEC.},
	author = {Couey, Jonathan J and Witoelar, Aree and Zhang, Sheng-Jia and Zheng, Kang and Ye, Jing and Dunn, Benjamin and Czajkowski, Rafal and Moser, May-Britt and Moser, Edvard I and Roudi, Yasser and Witter, Menno P},
	da = {2013/03/01},
	date-added = {2021-11-18 13:43:53 +0100},
	date-modified = {2021-11-18 13:43:53 +0100},
	doi = {10.1038/nn.3310},
	id = {Couey2013},
	isbn = {1546-1726},
	journal = {Nature Neuroscience},
	number = {3},
	pages = {318--324},
	title = {Recurrent inhibitory circuitry as a mechanism for grid formation},
	ty = {JOUR},
	url = {https://doi.org/10.1038/nn.3310},
	volume = {16},
	year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1038/nn.3310}}

@ARTICLE{PredictivePlaceCell,
	
	AUTHOR={Gönner, Lorenz and Vitay, Julien and Hamker, Fred H.},   
	
	TITLE={Predictive Place-Cell Sequences for Goal-Finding Emerge from Goal Memory and the Cognitive Map: A Computational Model},      
	
	JOURNAL={Frontiers in Computational Neuroscience},      
	
	VOLUME={11},      
	
	PAGES={84},     
	
	YEAR={2017},      
	
	URL={https://www.frontiersin.org/article/10.3389/fncom.2017.00084},       
	
	DOI={10.3389/fncom.2017.00084},      
	
	ISSN={1662-5188},   
	
	ABSTRACT={Hippocampal place-cell sequences observed during awake immobility often represent previous experience, suggesting a role in memory processes. However, recent reports of goals being overrepresented in sequential activity suggest a role in short-term planning, although a detailed understanding of the origins of hippocampal sequential activity and of its functional role is still lacking. In particular, it is unknown which mechanism could support efficient planning by generating place-cell sequences biased toward known goal locations, in an adaptive and constructive fashion. To address these questions, we propose a model of spatial learning and sequence generation as interdependent processes, integrating cortical contextual coding, synaptic plasticity and neuromodulatory mechanisms into a map-based approach. Following goal learning, sequential activity emerges from continuous attractor network dynamics biased by goal memory inputs. We apply Bayesian decoding on the resulting spike trains, allowing a direct comparison with experimental data. Simulations show that this model (1) explains the generation of never-experienced sequence trajectories in familiar environments, without requiring virtual self-motion signals, (2) accounts for the bias in place-cell sequences toward goal locations, (3) highlights their utility in flexible route planning, and (4) provides specific testable predictions.}
}

@article{Temporal_binding,
	title = {Temporal binding within and across events},
	journal = {Neurobiology of Learning and Memory},
	volume = {134},
	pages = {107-114},
	year = {2016},
	note = {Hippocampal Interactions with Brain Networks that Influence Learning  Memory},
	issn = {1074-7427},
	doi = {https://doi.org/10.1016/j.nlm.2016.07.011},
	url = {https://www.sciencedirect.com/science/article/pii/S1074742716301113},
	author = {Sarah DuBrow and Lila Davachi},
	keywords = {Order memory, Episodic memory, Event segmentation, Temporal context, MTL},
	abstract = {Remembering the order in which events occur is a fundamental component of episodic memory. However, the neural mechanisms supporting serial recall remain unclear. Behaviorally, serial recall is greater for information encountered within the same event compared to across event boundaries, raising the possibility that contextual stability may modulate the cognitive and neural processes supporting serial encoding. In the present study, we used fMRI during the encoding of consecutive face and object stimuli to elucidate the neural encoding signatures supporting subsequent serial recall behavior both within and across events. We found that univariate BOLD activation in both the middle hippocampus and left ventrolateral prefrontal cortex (PFC) was associated with subsequent serial recall of items that occur across event boundaries. By contrast, successful serial encoding within events was associated with increased functional connectivity between the hippocampus and ventromedial PFC, but not with univariate activation in these or other regions. These findings build on evidence implicating hippocampal and PFC processes in encoding temporal aspects of memory. They further suggest that these encoding processes are influenced by whether binding occurs within a stable context or bridges two adjacent but distinct events.}
}

@article{space_in_the_brain,
	title = {The representation of space in the brain},
	journal = {Behavioural Processes},
	volume = {135},
	pages = {113-131},
	year = {2017},
	issn = {0376-6357},
	doi = {https://doi.org/10.1016/j.beproc.2016.12.012},
	url = {https://www.sciencedirect.com/science/article/pii/S0376635716302480},
	author = {Roddy M. Grieves and Kate J. Jeffery},
	keywords = {Spatial cognition, Navigation, Place cell, Grid cell, Head direction cell},
	abstract = {Animals can navigate vast distances and often display behaviours or activities that indicate a detailed, internal spatial representation of their surrounding environment or a ‘cognitive map’. Over a century of behavioural research on spatial navigation in humans and animals has greatly increased our understanding of how this highly complex feat is achieved. In turn this has inspired half a century of electrophysiological spatial navigation and memory research which has further advanced our understanding of the brain. In particular, three functional cell types have been suggested to underlie cognitive mapping processes; place cells, head direction cells and grid cells. However, there are numerous other spatially modulated neurons in the brain. For a more complete understanding of the electrophysiological systems and behavioural processes underlying spatial navigation we must also examine these lesser understood neurons. In this review we will briefly summarise the literature surrounding place cells, head direction cells, grid cells and the evidence that these cells collectively form the neural basis of a cognitive map. We will then review literature covering many other spatially modulated neurons in the brain that perhaps further augment this cognitive map.}
}

@article{hippocampal_indexing_theory,
	title={The hippocampal indexing theory and episodic memory: Updating the index},
	author={Timothy James Teyler and Jerry W. Rudy},
	journal={Hippocampus},
	year={2007},
	volume={17}
}
@article{model_of_spatial_and_episodic_memory,
	title={A unified model of spatial and episodic memory},
	author={E. T. Rolls and Simon Maitland Stringer and Thomas P. Trappenberg},
	journal={Proceedings of the Royal Society of London. Series B: Biological Sciences},
	year={2002},
	volume={269},
	pages={1087 - 1093}
}

@article{neural_basis_of_the_cognitive_map,
	abstract = {Accumulating evidence indicates that the foundation of mammalian spatial orientation and learning is based on an internal network that can keep track of relative position and orientation (from an arbitrary starting point) on the basis of integration of self-motion cues derived from locomotion, vestibular activation and optic flow (path integration).Place cells in the hippocampal formation exhibit elevated activity at discrete spots in a given environment, and this spatial representation is determined primarily on the basis of which cells were active at the starting point and how far and in what direction the animal has moved since then. Environmental features become associatively bound to this intrinsic spatial framework and can serve to correct for cumulative error in the path integration process.Theoretical studies suggested that a path integration system could involve cooperative interactions (attractor dynamics) among a population of place coding neurons, the synaptic coupling of which defines a two-dimensional attractor map. These cells would communicate with an additional group of neurons, the activity of which depends on the conjunction of movement speed, location and orientation (head direction) information, allowing position on the attractor map to be updated by self-motion information.The attractor map hypothesis contains an inherent boundary problem: what happens when the animal's movements carry it beyond the boundary of the map? One solution to this problem is to make the boundaries of the map periodic by coupling neurons at each edge to those on the opposite edge, resulting in a toroidal synaptic matrix. This solution predicts that, in a sufficiently large space, place cells would exhibit a regularly spaced grid of place fields, something that has never been observed in the hippocampus proper.Recent discoveries in layer II of the medial entorhinal cortex (MEC), the main source of hippocampal afferents, indicate that these cells do have regularly spaced place fields (grid cells). In addition, cells in the deeper layers of this structure exhibit grid fields that are conjunctive for head orientation and movement speed. Pure head direction neurons are also found there. Therefore, all of the components of previous theoretical models for path integration appear in the MEC, suggesting that this network is the core of the path integration system.The scale of MEC spatial firing grids increases systematically from the dorsal to the ventral poles of this structure, in much the same way as is observed for hippocampal place cells, and we show how non-periodic hippocampal place fields could arise from the combination of inputs from entorhinal grid cells, if the inputs cover a range of spatial scales rather than a single scale. This phenomenon, in the spatial domain, is analogous to the low frequency 'beats' heard when two pure tones of slightly different frequencies are combined.The problem of how a two-dimensional synaptic matrix with periodic boundary conditions, postulated to underlie grid cell behaviour, could be self-organized in early development is addressed. Based on principles derived from Alan Turing's theory of spontaneous symmetry breaking in chemical systems, we suggest that topographically organized, grid-like patterns of neural activity might be present in the immature cortex, and that these activity patterns guide the development of the proposed periodic synaptic matrix through a mechanism involving competitive synaptic plasticity.},
	author = {McNaughton, Bruce L. and Battaglia, Francesco P. and Jensen, Ole and Moser, Edvard I and Moser, May-Britt},
	da = {2006/08/01},
	date-added = {2021-11-18 13:57:08 +0100},
	date-modified = {2021-11-18 13:57:08 +0100},
	doi = {10.1038/nrn1932},
	id = {McNaughton2006},
	isbn = {1471-0048},
	journal = {Nature Reviews Neuroscience},
	number = {8},
	pages = {663--678},
	title = {Path integration and the neural basis of the 'cognitive map'},
	ty = {JOUR},
	url = {https://doi.org/10.1038/nrn1932},
	volume = {7},
	year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1038/nrn1932}}


@article{Object_vector_coding,
	abstract = {The hippocampus and the medial entorhinal cortex are part of a brain system that maps self-location during navigation in the proximal environment1,2. In this system, correlations between neural firing and an animal's position or orientation are so evident that cell types have been given simple descriptive names, such as place cells3, grid cells4, border cells5,6 and head-direction cells7. While the number of identified functional cell types is growing at a steady rate, insights remain limited by an almost-exclusive reliance on recordings from rodents foraging in empty enclosures that are different from the richly populated, geometrically irregular environments of the natural world. In environments that contain discrete objects, animals are known to store information about distance and direction to those objects and to use this vector information to guide navigation8--10. Theoretical studies have proposed that such vector operations are supported by neurons that use distance and direction from discrete objects11,12 or boundaries13,14 to determine the animal's location, but---although some cells with vector-coding properties may be present in the hippocampus15 and subiculum16,17---it remains to be determined whether and how vectorial operations are implemented in the wider neural representation of space. Here we show that a large fraction of medial entorhinal cortex neurons fire specifically when mice are at given distances and directions from spatially confined objects. These `object-vector cells'are tuned equally to a spectrum of discrete objects, irrespective of their location in the test arena, as well as to a broad range of dimensions and shapes, from point-like objects to extended surfaces. Our findings point to vector coding as a predominant form of position coding in the medial entorhinal cortex.},
	author = {H{\o}ydal, {\O}yvind Arne and Skyt{\o}en, Emilie Ranheim and Andersson, Sebastian Ola and Moser, May-Britt and Moser, Edvard I.},
	da = {2019/04/01},
	date-added = {2021-11-18 14:02:45 +0100},
	date-modified = {2021-11-18 14:02:45 +0100},
	doi = {10.1038/s41586-019-1077-7},
	id = {H{\o}ydal2019},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7752},
	pages = {400--404},
	title = {Object-vector coding in the medial entorhinal cortex},
	ty = {JOUR},
	url = {https://doi.org/10.1038/s41586-019-1077-7},
	volume = {568},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41586-019-1077-7}}

@article{coding_of_space_and_time,
	title={Neurophysiological coding of space and time in the hippocampus, entorhinal cortex, and retrosplenial cortex},
	author={Andrew S. Alexander and Jennifer C. Robinson and Holger Dannenberg and Nathaniel R. Kinsky and Samuel J Levy and William Mau and George W Chapman and David W. Sullivan and Michael E. Hasselmo},
	journal={Brain and Neuroscience Advances},
	year={2020},
	volume={4}
}

@article{Neuronal_vector_coding_in_spatial_cognition,
	abstract = {Several types of neurons involved in spatial navigation and memory encode the distance and direction (that is, the vector) between an agent and items in its environment. Such vectorial information provides a powerful basis for spatial cognition by representing the geometric relationships between the self and the external world. Here, we review the explicit encoding of vectorial information by neurons in and around the hippocampal formation, far from the sensory periphery. The parahippocampal, retrosplenial and parietal cortices, as well as the hippocampal formation and striatum, provide a plethora of examples of vector coding at the single neuron level. We provide a functional taxonomy of cells with vectorial receptive fields as reported in experiments and proposed in theoretical work. The responses of these neurons may provide the fundamental neural basis for the (bottom-up) representation of environmental layout and (top-down) memory-guided generation of visuospatial imagery and navigational planning.},
	author = {Bicanski, Andrej and Burgess, Neil},
	da = {2020/09/01},
	date-added = {2021-11-18 14:08:52 +0100},
	date-modified = {2021-11-18 14:08:52 +0100},
	doi = {10.1038/s41583-020-0336-9},
	id = {Bicanski2020},
	isbn = {1471-0048},
	journal = {Nature Reviews Neuroscience},
	number = {9},
	pages = {453--470},
	title = {Neuronal vector coding in spatial cognition},
	ty = {JOUR},
	url = {https://doi.org/10.1038/s41583-020-0336-9},
	volume = {21},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41583-020-0336-9}}


@article{Geometry_of_abstract,
	abstract = {Hippocampal neurons encode physical variables1--7 such as space1 or auditory frequency6 in cognitive maps8. In addition, functional magnetic resonance imaging studies in humans have shown that the hippocampus can also encode more abstract, learned variables9--11. However, their integration into existing neural representations of physical variables12,13 is unknown. Here, using two-photon calcium imaging, we show that individual neurons in the dorsal hippocampus jointly encode accumulated evidence with spatial position in mice performing a decision-making task in virtual reality14--16. Nonlinear dimensionality reduction13 showed that population activity was well-described by approximately four to six latent variables, which suggests that neural activity is constrained to a low-dimensional manifold. Within this low-dimensional space, both physical and abstract variables were jointly mapped in an orderly manner, creating a geometric representation that we show is similar across mice. The existence of conjoined cognitive maps suggests that the hippocampus performs a general computation---the creation of task-specific low-dimensional manifolds that contain a geometric representation of learned knowledge.},
	author = {Nieh, Edward H. and Schottdorf, Manuel and Freeman, Nicolas W. and Low, Ryan J. and Lewallen, Sam and Koay, Sue Ann and Pinto, Lucas and Gauthier, Jeffrey L. and Brody, Carlos D. and Tank, David W.},
	da = {2021/07/01},
	date-added = {2021-11-18 14:11:18 +0100},
	date-modified = {2021-11-18 14:11:18 +0100},
	doi = {10.1038/s41586-021-03652-7},
	id = {Nieh2021},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7865},
	pages = {80--84},
	title = {Geometry of abstract learned knowledge in the hippocampus},
	ty = {JOUR},
	url = {https://doi.org/10.1038/s41586-021-03652-7},
	volume = {595},
	year = {2021},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41586-021-03652-7}}





@article{Spatial_Representations_of_Granule_Cells,
	title={Spatial Representations of Granule Cells and Mossy Cells of the Dentate Gyrus},
	author={Douglas GoodSmith and Xiaojing Chen and Cheng Wang and Sang Hoon Kim and Hongjun Song and Andrea Burgalossi and Kimberly M. Christian and James J. Knierim},
	journal={Neuron},
	year={2017},
	volume={93},
	pages={677-690.e5}
}

@article{Pattern_separation_of_spiketrains,
	abstract = {Pattern separation is a process that minimizes overlap between patterns of neuronal activity representing similar experiences. Theoretical work suggests that the dentate gyrus (DG) performs this role for memory processing but a direct demonstration is lacking. One limitation is the difficulty to measure DG inputs and outputs simultaneously. To rigorously assess pattern separation by DG circuitry, we used mouse brain slices to stimulate DG afferents and simultaneously record DG granule cells (GCs) and interneurons. Output spiketrains of GCs are more dissimilar than their input spiketrains, demonstrating for the first time temporal pattern separation at the level of single neurons in the DG. Pattern separation is larger in GCs than in fast-spiking interneurons and hilar mossy cells, and is amplified in CA3 pyramidal cells. Analysis of the neural noise and computational modelling suggest that this form of pattern separation is not explained by simple randomness and arises from specific presynaptic dynamics. Overall, by reframing the concept of pattern separation in dynamic terms and by connecting it to the physiology of different types of neurons, our study offers a new window of understanding in how hippocampal networks might support episodic memory.},
	author = {Madar, Antoine D. and Ewell, Laura A. and Jones, Mathew V.},
	da = {2019/03/27},
	date-added = {2021-11-18 14:20:24 +0100},
	date-modified = {2021-11-18 14:20:24 +0100},
	doi = {10.1038/s41598-019-41503-8},
	id = {Madar2019},
	isbn = {2045-2322},
	journal = {Scientific Reports},
	number = {1},
	pages = {5282},
	title = {Pattern separation of spiketrains in hippocampal neurons},
	ty = {JOUR},
	url = {https://doi.org/10.1038/s41598-019-41503-8},
	volume = {9},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41598-019-41503-8}}



@article{boundary_vector_cells,
	author = {Roddy M Grieves and Éléonore Duvelle and Paul A Dudchenko},
	title = {A boundary vector cell model of place field repetition},
	journal = {Spatial Cognition \& Computation},
	volume = {18},
	number = {3},
	pages = {217-256},
	year  = {2018},
	publisher = {Taylor & Francis},
	doi = {10.1080/13875868.2018.1437621},
	
	URL = { 
	https://doi.org/10.1080/13875868.2018.1437621
	
	},
	eprint = { 
	https://doi.org/10.1080/13875868.2018.1437621
	
	}
	
}


@unpublished{bami,
	title={Biological and Machine Intelligence (BAMI)},
	author={Hawkins, J. and Ahmad, S. and Purdy, S. and Lavin, A.},
	note={Initial online release 0.4},
	url={https://numenta.com/resources/biological-and-machine-intelligence/},
	year={2016}
}
@article{Encoding_Data_for_HTM,
	title={Encoding Data for HTM Systems},
	author={Scott Purdy},
	journal={ArXiv},
	year={2016},
	volume={abs/1602.05925}
}
@ARTICLE{spacial_pooler,
	
	AUTHOR={Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},   
	
	TITLE={The HTM Spatial Pooler—A Neocortical Algorithm for Online Sparse Distributed Coding},      
	
	JOURNAL={Frontiers in Computational Neuroscience},      
	
	VOLUME={11},      
	
	PAGES={111},     
	
	YEAR={2017},      
	
	URL={https://www.frontiersin.org/article/10.3389/fncom.2017.00111},       
	
	DOI={10.3389/fncom.2017.00111},      
	
	ISSN={1662-5188},   
	
	ABSTRACT={Hierarchical temporal memory (HTM) provides a theoretical framework that models several key computational principles of the neocortex. In this paper, we analyze an important component of HTM, the HTM spatial pooler (SP). The SP models how neurons learn feedforward connections and form efficient representations of the input. It converts arbitrary binary input patterns into sparse distributed representations (SDRs) using a combination of competitive Hebbian learning rules and homeostatic excitability control. We describe a number of key properties of the SP, including fast adaptation to changing input statistics, improved noise robustness through learning, efficient use of cells, and robustness to cell death. In order to quantify these properties we develop a set of metrics that can be directly computed from the SP outputs. We show how the properties are met using these metrics and targeted artificial simulations. We then demonstrate the value of the SP in a complete end-to-end real-world HTM system. We discuss the relationship with neuroscience and previous studies of sparse coding. The HTM spatial pooler represents a neurally inspired algorithm for learning sparse representations from noisy data streams in an online fashion.}
}
@article{htm_sequence_learning,
	author = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
	title = {Continuous Online Sequence Learning with an Unsupervised Neural Network Model},
	year = {2016},
	issue_date = {November 2016},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
	volume = {28},
	number = {11},
	issn = {0899-7667},
	doi = {10.1162/NECO_a_00893},
	abstract = {The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory HTM sequence memory recently has been proposed as a theoretical framework for sequence learning in the cortex. In this letter, we analyze properties of HTM sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory with other sequence learning algorithms, including statistical methods-autoregressive integrated moving average; feedforward neural networks-time delay neural network and online sequential extreme learning machine; and recurrent neural networks-long short-term memory and echo-state networks on sequence prediction problems with both artificial and real-world data. The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning. Therefore, the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem but is also applicable to real-world sequence learning problems from continuous data streams.},
	journal = {Neural Comput.},
	month = {nov},
	pages = {2474–2504},
	numpages = {31}
}





@article{future-paths,
	abstract = {Effective navigation requires planning extended routes to remembered goal locations. Hippocampal place cells have been proposed to have a role in navigational planning, but direct evidence has been lacking. Here we show that before goal-directed navigation in an open arena, the rat hippocampus generates brief sequences encoding spatial trajectories strongly biased to progress from the subject's current location to a known goal location. These sequences predict immediate future behaviour, even in cases in which the specific combination of start and goal locations is novel. These results indicate that hippocampal sequence events characterized previously in linearly constrained environments as `replay'are also capable of supporting a goal-directed, trajectory-finding mechanism, which identifies important places and relevant behavioural paths, at specific times when memory retrieval is required, and in a manner that could be used to control subsequent navigational behaviour.},
	author = {Pfeiffer, Brad E. and Foster, David J.},
	da = {2013/05/01},
	date-added = {2021-11-18 21:03:38 +0100},
	date-modified = {2021-11-18 21:03:38 +0100},
	doi = {10.1038/nature12112},
	id = {Pfeiffer2013},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7447},
	pages = {74--79},
	title = {Hippocampal place-cell sequences depict future paths to remembered goals},
	ty = {JOUR},
	url = {https://doi.org/10.1038/nature12112},
	volume = {497},
	year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1038/nature12112}}



@article{swr_correct_and_error,
	abstract = {Theta rhythms temporally coordinate sequences of hippocampal place cell ensembles during active behaviors, while sharp wave-ripples coordinate place cell sequences during rest. We investigated whether such coordination of hippocampal place cell sequences is disrupted during error trials in a delayed match-to-place task. As a reward location was learned across trials, place cell sequences developed that represented temporally compressed paths to the reward location during the approach to the reward location. Less compressed paths were represented on error trials as an incorrect stop location was approached. During rest periods of correct but not error trials, place cell sequences developed a bias to replay representations of paths ending at the correct reward location. These results support the hypothesis that coordination of place cell sequences by theta rhythms and sharp wave-ripples develops as a reward location is learned and may be important for the successful performance of a spatial memory task.},
	author = {Zheng, Chenguang and Hwaun, Ernie and Loza, Carlos A. and Colgin, Laura Lee},
	da = {2021/06/07},
	date-added = {2021-11-18 21:06:07 +0100},
	date-modified = {2021-11-18 21:06:07 +0100},
	doi = {10.1038/s41467-021-23765-x},
	id = {Zheng2021},
	isbn = {2041-1723},
	journal = {Nature Communications},
	number = {1},
	pages = {3373},
	title = {Hippocampal place cell sequences differ during correct and error trials in a spatial memory task},
	ty = {JOUR},
	url = {https://doi.org/10.1038/s41467-021-23765-x},
	volume = {12},
	year = {2021},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41467-021-23765-x}}

@ARTICLE{Locations_in_the_Neocortex,
	
	AUTHOR={Lewis, Marcus and Purdy, Scott and Ahmad, Subutai and Hawkins, Jeff},   
	
	TITLE={Locations in the Neocortex: A Theory of Sensorimotor Object Recognition Using Cortical Grid Cells},      
	
	JOURNAL={Frontiers in Neural Circuits},      
	
	VOLUME={13},      
	
	PAGES={22},     
	
	YEAR={2019},      
	
	URL={https://www.frontiersin.org/article/10.3389/fncir.2019.00022},       
	
	DOI={10.3389/fncir.2019.00022},      
	
	ISSN={1662-5110},   
	
	ABSTRACT={The neocortex is capable of anticipating the sensory results of movement but the neural mechanisms are poorly understood. In the entorhinal cortex, grid cells represent the location of an animal in its environment, and this location is updated through movement and path integration. In this paper, we propose that sensory neocortex incorporates movement using grid cell-like neurons that represent the location of sensors on an object. We describe a two-layer neural network model that uses cortical grid cells and path integration to robustly learn and recognize objects through movement and predict sensory stimuli after movement. A layer of cells consisting of several grid cell-like modules represents a location in the reference frame of a specific object. Another layer of cells which processes sensory input receives this location input as context and uses it to encode the sensory input in the object’s reference frame. Sensory input causes the network to invoke previously learned locations that are consistent with the input, and motor input causes the network to update those locations. Simulations show that the model can learn hundreds of objects even when object features alone are insufficient for disambiguation. We discuss the relationship of the model to cortical circuitry and suggest that the reciprocal connections between layers 4 and 6 fit the requirements of the model. We propose that the subgranular layers of cortical columns employ grid cell-like mechanisms to represent object specific locations that are updated through movement.}
}


@article{Pattern_separation,
	abstract = {Pattern separation is a process that minimizes overlap between patterns of neuronal activity representing similar experiences. Theoretical work suggests that the dentate gyrus (DG) performs this role for memory processing but a direct demonstration is lacking. One limitation is the difficulty to measure DG inputs and outputs simultaneously. To rigorously assess pattern separation by DG circuitry, we used mouse brain slices to stimulate DG afferents and simultaneously record DG granule cells (GCs) and interneurons. Output spiketrains of GCs are more dissimilar than their input spiketrains, demonstrating for the first time temporal pattern separation at the level of single neurons in the DG. Pattern separation is larger in GCs than in fast-spiking interneurons and hilar mossy cells, and is amplified in CA3 pyramidal cells. Analysis of the neural noise and computational modelling suggest that this form of pattern separation is not explained by simple randomness and arises from specific presynaptic dynamics. Overall, by reframing the concept of pattern separation in dynamic terms and by connecting it to the physiology of different types of neurons, our study offers a new window of understanding in how hippocampal networks might support episodic memory.},
	author = {Madar, Antoine D. and Ewell, Laura A. and Jones, Mathew V.},
	da = {2019/03/27},
	date-added = {2021-11-20 20:56:48 +0100},
	date-modified = {2021-11-20 20:56:48 +0100},
	doi = {10.1038/s41598-019-41503-8},
	id = {Madar2019},
	isbn = {2045-2322},
	journal = {Scientific Reports},
	number = {1},
	pages = {5282},
	title = {Pattern separation of spiketrains in hippocampal neurons},
	ty = {JOUR},
	url = {https://doi.org/10.1038/s41598-019-41503-8},
	volume = {9},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41598-019-41503-8}}

@article{Spatial_view_cells,
	title={Spatial view cells and the representation of place in the primate hippocampus},
	author={Edmund T. Rolls},
	journal={Hippocampus},
	year={1999},
	volume={9}
}

@article{Place_Cell_Like_Activity_in_the_Primary_Sensorimotor,
	abstract = {Primary motor (M1), primary somatosensory (S1) and dorsal premotor (PMd) cortical areas of rhesus monkeys previously have been associated only with sensorimotor control of limb movements. Here we show that a significant number of neurons in these areas also represent body position and orientation in space. Two rhesus monkeys (K and M) used a wheelchair controlled by a brain-machine interface (BMI) to navigate in a room. During this whole-body navigation, the discharge rates of M1, S1, and PMd neurons correlated with the two-dimensional (2D) room position and the direction of the wheelchair and the monkey head. This place cell-like activity was observed in both monkeys, with 44.6{\%} and 33.3{\%} of neurons encoding room position in monkeys K and M, respectively, and the overlapping populations of 41.0{\%} and 16.0{\%} neurons encoding head direction. These observations suggest that primary sensorimotor and premotor cortical areas in primates are likely involved in allocentrically representing body position in space during whole-body navigation, which is an unexpected finding given the classical hierarchical model of cortical processing that attributes functional specialization for spatial processing to the hippocampal formation.},
	author = {Yin, A. and Tseng, P. H. and Rajangam, S. and Lebedev, M. A. and Nicolelis, M. A. L.},
	da = {2018/06/15},
	date-added = {2021-11-22 08:33:42 +0100},
	date-modified = {2021-11-22 08:33:42 +0100},
	doi = {10.1038/s41598-018-27472-4},
	id = {Yin2018},
	isbn = {2045-2322},
	journal = {Scientific Reports},
	number = {1},
	pages = {9184},
	title = {Place Cell-Like Activity in the Primary Sensorimotor and Premotor Cortex During Monkey Whole-Body Navigation},
	ty = {JOUR},
	url = {https://doi.org/10.1038/s41598-018-27472-4},
	volume = {8},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41598-018-27472-4}}

@article{Polychronization,
	author = {Izhikevich, Eugene M.},
	title = {Polychronization: Computation with Spikes},
	year = {2006},
	issue_date = {February 2006},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
	volume = {18},
	number = {2},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/089976606775093882},
	doi = {10.1162/089976606775093882},
	abstract = {We present a minimal spiking network that can polychronize, that is, exhibit reproducible time-locked but not synchronous firing patterns with millisecond precision, as in synfire braids. The network consists of cortical spiking neurons with axonal conduction delays and spike-timing-dependent plasticity (STDP); a ready-to-use MATLAB code is included. It exhibits sleeplike oscillations, gamma (40 Hz) rhythms, conversion of firing rates to spike timings, and other interesting regimes. Due to the interplay between the delays and STDP, the spiking neurons spontaneously self-organize into groups and generate patterns of stereotypical polychronous activity. To our surprise, the number of coexisting polychronous groups far exceeds the number of neurons in the network, resulting in an unprecedented memory capacity of the system. We speculate on the significance of polychrony to the theory of neuronal group selection (TNGS, neural Darwinism), cognitive neural computations, binding and gamma rhythm, mechanisms of attention, and consciousness as "attention to memories."},
	journal = {Neural Comput.},
	month = {feb},
	pages = {245–282},
	numpages = {38}
}

